#!venv/bin/python

"""
Optimize parameters of a search template using the rank evaluation API for
measurements. This is a black-box optimization problem where we treat
Elasticsearch and a relevance metric as a non-convex function to optimize the
parameter space over. We use either basic grid or random search techniques to
explore the parameter space, or for larger parameter spaces we use more advanced
techniques such as Bayesian optimization.

This approach is dependent on Elasticsearch and specifically the rank evaluation
API, and we use {{scikit-learn}} and {{scikit-optimize}} for finding the optimal
parameters.
"""

import argparse
import json
import sys
import os

from elasticsearch import Elasticsearch
from sklearn.model_selection import ParameterGrid
from skopt import gp_minimize
from skopt.callbacks import DeltaXStopper

# project library
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from qopt.eval import build_requests
from qopt.optimize import Config, merge_param_train
from qopt.trec import load_qrels, load_queries_as_tuple
from qopt.util import Timer, load_json

DEFAULT_METRIC = os.path.join('config', 'metric-mrr.json')
DEFAULT_PARAMS = os.path.join('config', 'params-defaults.json')
DEFAULT_TEMPLATE_ID = 'template'
DEFAULT_URL = 'http://elastic:changeme@localhost:9200'


def search_and_evaluate(es, index, metric, templates, template_id, queries, qrels, param_train):
    """Run the rank evaluation API which will search all requests and evaluate based on the provided resources."""

    # build the rank eval API request body from components
    params = merge_param_train(param_train)
    requests = build_requests(index, template_id, queries, qrels, params)
    body = {
        'metric': metric,
        'templates': templates,
        'requests': requests,
    }

    results = es.rank_eval(index=index, body=body, request_timeout=1200,
                           allow_no_indices=False, ignore_unavailable=False)
    return results['metric_score']


def main():
    parser = argparse.ArgumentParser(prog='optimize-query')
    parser.add_argument('--url', default=DEFAULT_URL,
                        help="An Elasticsearch connection URL, e.g. http://user:secret@localhost:9200")
    parser.add_argument('--index', required=True, help="The index name to use")
    parser.add_argument('--metric', default=DEFAULT_METRIC,
                        help=f"A JSON file containing the rank eval metric definition. Default: {DEFAULT_METRIC}.")
    parser.add_argument('--templates', required=True, help="A JSON file containing search templates to use")
    parser.add_argument('--template-id', default=DEFAULT_TEMPLATE_ID,
                        help=f"The template ID of the template to use for all requests. Default: {DEFAULT_TEMPLATE_ID}.")
    parser.add_argument('--queries', required=True,
                        help="The TREC Topic file with the queries that correspond to the 'qrels' file")
    parser.add_argument('--qrels', required=True, help="The TREC QRELs file corresponding to the 'queries' file")
    parser.add_argument('--config', required=True,
                        help="A file containing the configuration for the parameters to optimize.")
    args = parser.parse_args()

    es = Elasticsearch(args.url)

    metric = load_json(args.metric)
    templates = load_json(args.templates)
    queries = list(load_queries_as_tuple(args.queries))
    qrels = load_qrels(args.qrels)
    config = Config.parse(load_json(args.config))

    print("Using configuration")
    print(f" - metric: {json.dumps(metric, indent=2)}")
    print()

    param_train = [config.default]
    for config_space in config.spaces:
        base_params = merge_param_train(param_train + [config_space.default])
        selected_method = config_space.select_method()

        print(f"Running optimization for space: {config_space.name}")
        print(f" - selected method: {selected_method}")
        if selected_method != 'grid':
            print(f" - num iterations: {config_space.num_iterations}")
        print(f" - base params: {json.dumps(base_params)}")

        def params_with_values(param_values):
            return {k: v for k, v in zip(config_space.dimension_names(), param_values)}

        def objective_fn(param_values):
            params = params_with_values(param_values)
            return -1 * search_and_evaluate(
                es, args.index, metric, templates, args.template_id, queries, qrels,
                param_train=[base_params, params])

        def logger(result):
            x0 = result.x_iters  # list of input points
            y0 = result.func_vals  # evaluation of input points

            num_iters = len(x0)
            params = params_with_values(x0[-1])
            score = -1*y0[-1]

            print(f"   - iteration {num_iters} evaluated: {json.dumps(params)}")
            print(f"     - score: {score:.04f}")

        best_params = {}
        best_score = 0.0
        with Timer() as t:
            if selected_method == 'grid':
                grid_space = params_with_values([list(dim.categories) for dim in config_space.space])
                for i, params in enumerate(list(ParameterGrid(grid_space))):
                    score = search_and_evaluate(
                        es, args.index, metric, templates, args.template_id, queries, qrels,
                        param_train=[base_params, params])
                    print(f"   - iteration {i+1} evaluated: {json.dumps(params)}")
                    print(f"     - score: {score:.04f}")
                    if score > best_score:
                        best_score = score
                        best_params = params.copy()

            elif selected_method == 'bayesian':
                res = gp_minimize(func=objective_fn, dimensions=config_space.space,
                                  n_calls=30,  # total calls to func, includes initial points
                                  n_initial_points=5,
                                  initial_point_generator='random',
                                  acq_func='PI',  # automatic at each iteration
                                  acq_optimizer='auto',  # selects the optimizer based on dimension types
                                  xi=0.01,  # improvement expected for acquisition method "EI" or "PI"
                                  kappa=4.0,  # exploration/exploitation for acquisition method "LCB", higher means more exploration
                                  verbose=False,
                                  callback=[DeltaXStopper(1e-8), logger])
                best_params = params_with_values(res.x)
                best_score = -1*res.fun
            else:
                raise ValueError(f"Unsupported method: {selected_method}")

        param_train.append(best_params)
        print(f" - best params: {best_params}")
        print(f" - best score: {best_score:.04f}")
        print(f" - duration: {t.interval_str()}")
        print()

    print("Final params:")
    print(merge_param_train(param_train))


if __name__ == "__main__":
    main()
